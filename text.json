[
    "The LangChain\necosystem\nDEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nJames Chapman\nCurriculum Manager, DataCamp",
    "DataCamp",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nMeet your instructors...\n \n  \n \nJonathan Bennion, AI Engineer\nML & AI at Facebook, Google, Amazon,\nDisney, EA\nCreated Logical Fallacy chain in LangChain\nCont",
    "neer\nML & AI at Facebook, Google, Amazon,\nDisney, EA\nCreated Logical Fallacy chain in LangChain\nContributor to DeepEval",
    "ributor to DeepEval",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nBuild LLM Apps with LangChain",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nLangChain integrations\n https://python.langchain.com/docs/integrations/providers/1",
    "/integrations/providers/1",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nBuilding LLM apps the LangChain way...",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAINBuilding LLM apps the LangChain way...",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAINBuilding LLM apps the LangChain way...",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAINBuilding LLM apps the LangChain way...",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAINBuilding LLM apps the LangChain way...",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nPrompting OpenAI models\nfrom langchain_openai import ChatOpenAI  \nllm = ChatOpenAI( \n    model=\"gpt-4o-mini\", \n    api_key='...' \n)  \nllm.invoke(\"What is Lan",
    "OpenAI  \nllm = ChatOpenAI( \n    model=\"gpt-4o-mini\", \n    api_key='...' \n)  \nllm.invoke(\"What is LangChain?\") \nLangChain is a framework designed for developing applications... \nAdditional parameters:",
    "gChain?\") \nLangChain is a framework designed for developing applications... \nAdditional parameters: max_completion_tokens, temperature, etc.\n https://platform.openai.com/docs/quickstart1",
    "max_completion_tokens, temperature, etc.\n https://platform.openai.com/docs/quickstart1",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\n\u0000 Prompting Hugging Face models\nfrom langchain_huggingface import HuggingFacePipeline  \nllm = HuggingFacePipeline.from_model_id( \n    model_id=\"meta-llama/Ll",
    "e import HuggingFacePipeline  \nllm = HuggingFacePipeline.from_model_id( \n    model_id=\"meta-llama/Llama-3.2-3B-Instruct\", \n    task=\"text-generation\", \n    pipeline_kwargs={\"max_new_tokens\": 100} \n)",
    "ama-3.2-3B-Instruct\", \n    task=\"text-generation\", \n    pipeline_kwargs={\"max_new_tokens\": 100} \n)  \nllm.invoke(\"What is Hugging Face?\") \nHugging Face is a popular open-source artificial intelligence",
    "llm.invoke(\"What is Hugging Face?\") \nHugging Face is a popular open-source artificial intelligence (AI) library...",
    "(AI) library...",
    "Let's practice!\nDEVELOPING LLM APPLICATIONS WITH LANGCHAIN",
    "Prompt templates\nDEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nJames Chapman\nCurriculum Manager, DataCamp",
    "mp",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nPrompt templates\nRecipes for defining prompts for LLMs\nCan contain: instructions, examples, and additional context",
    "n contain: instructions, examples, and additional context",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nPrompt templates\nfrom langchain_core.prompts import PromptTemplate \n \ntemplate = \"Expain this concept simply and concisely: {concept}\"  \nprompt_template = Pr",
    "tTemplate \n \ntemplate = \"Expain this concept simply and concisely: {concept}\"  \nprompt_template = PromptTemplate.from_template( \n    template=template \n)  \nprompt = prompt_template.invoke({\"concept\":",
    "omptTemplate.from_template( \n    template=template \n)  \nprompt = prompt_template.invoke({\"concept\": \"Prompting LLMs\"}) \nprint(prompt) \ntext='Expain this concept simply and concisely: Prompting LLMs'",
    "\"Prompting LLMs\"}) \nprint(prompt) \ntext='Expain this concept simply and concisely: Prompting LLMs'",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nllm = HuggingFacePipeline.from_model_id( \n    model_id=\"meta-llama/Llama-3.3-70B-Instruct\", \n    task=\"text-generation\" \n)  \nllm_chain = prompt_template | ll",
    "eta-llama/Llama-3.3-70B-Instruct\", \n    task=\"text-generation\" \n)  \nllm_chain = prompt_template | llm \n \nconcept = \"Prompting LLMs\" \nprint(llm_chain.invoke({\"concept\": concept})) \nPrompting LLMs (Larg",
    "m \n \nconcept = \"Prompting LLMs\" \nprint(llm_chain.invoke({\"concept\": concept})) \nPrompting LLMs (Large Language Models) refers to the process of giving a model a \nspecific input or question to generate",
    "e Language Models) refers to the process of giving a model a \nspecific input or question to generate a response. \nLangChain Expression Language (LCEL): | (pipe) operator\nChain: connect calls to differ",
    "a response. \nLangChain Expression Language (LCEL): | (pipe) operator\nChain: connect calls to different components",
    "ent components",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nChat models\nChat roles: system, human, ai\nfrom langchain_core.prompts import ChatPromptTemplate \n \ntemplate = ChatPromptTemplate.from_messages( \n    [",
    "core.prompts import ChatPromptTemplate \n \ntemplate = ChatPromptTemplate.from_messages( \n    [ \n        (\"system\", \"You are a calculator that responds with math.\"), \n        (\"human\", \"Answer this math",
    "(\"system\", \"You are a calculator that responds with math.\"), \n        (\"human\", \"Answer this math question: What is two plus two?\"), \n        (\"ai\", \"2+2=4\"), \n        (\"human\", \"Answer this math q",
    "question: What is two plus two?\"), \n        (\"ai\", \"2+2=4\"), \n        (\"human\", \"Answer this math question: {math}\") \n    ] \n)",
    "uestion: {math}\") \n    ] \n)",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nIntegrating ChatPromptTemplate\nllm = ChatOpenAI(model=\"gpt-4o-mini\", api_key='<OPENAI_API_TOKEN>') \n \nllm_chain = template | llm  \nmath='What is five times f",
    "t-4o-mini\", api_key='<OPENAI_API_TOKEN>') \n \nllm_chain = template | llm  \nmath='What is five times five?' \n \nresponse = llm_chain.invoke({\"math\": math}) \nprint(response.content) \n5x5=25",
    "ive?' \n \nresponse = llm_chain.invoke({\"math\": math}) \nprint(response.content) \n5x5=25",
    "Let's practice!\nDEVELOPING LLM APPLICATIONS WITH LANGCHAIN",
    "Few-shot prompting\nDEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nJames Chapman\nCurriculum Manager, DataCamp",
    "Camp",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nLimitations of standard prompt templates\n \nPromptTemplate + ChatPromptTemplate\n\u0000 Handling small numbers of examples\n\u0000 Don't scale to larger numbers\nFewShotPr",
    "+ ChatPromptTemplate\n\u0000 Handling small numbers of examples\n\u0000 Don't scale to larger numbers\nFewShotPromptTemplate \nexamples = [ \n    { \n        \"question\": \"...\" \n        \"answer\": \"...\" \n    }, \n    .",
    "omptTemplate \nexamples = [ \n    { \n        \"question\": \"...\" \n        \"answer\": \"...\" \n    }, \n    ... \n]",
    ".. \n]",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nBuilding an example set\nexamples = [ \n    { \n        \"question\": \"Does Henry Campbell have any pets?\", \n        \"answer\": \"Henry Campbell has a dog called Pl",
    "stion\": \"Does Henry Campbell have any pets?\", \n        \"answer\": \"Henry Campbell has a dog called Pluto.\" \n    }, \n    ... \n] \n# Convert pandas DataFrame to list of dicts \nexamples = df.to_dict(orient",
    "uto.\" \n    }, \n    ... \n] \n# Convert pandas DataFrame to list of dicts \nexamples = df.to_dict(orient=\"records\")",
    "=\"records\")",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nFormatting the examples\nfrom langchain_core.prompts import FewShotPromptTemplate, PromptTemplate  \nexample_prompt = PromptTemplate.from_template(\"Question: {",
    "t FewShotPromptTemplate, PromptTemplate  \nexample_prompt = PromptTemplate.from_template(\"Question: {question}\\n{answer}\") \nprompt = example_prompt.invoke({\"question\": \"What is the capital of Italy?\"",
    "question}\\n{answer}\") \nprompt = example_prompt.invoke({\"question\": \"What is the capital of Italy?\" \n                                \"answer\": \"Rome\"}) \nprint(prompt.text) \nQuestion: What is the capita",
    "\"answer\": \"Rome\"}) \nprint(prompt.text) \nQuestion: What is the capital of Italy? \nRome",
    "l of Italy? \nRome",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nFewShotPromptTemplate\n \nprompt_template = FewShotPromptTemplate(  \n    examples=examples,  \n    example_prompt=example_prompt,  \n    suffix=\"Question: {input",
    "mplate(  \n    examples=examples,  \n    example_prompt=example_prompt,  \n    suffix=\"Question: {input}\",  \n    input_variables=[\"input\"]  \n)  \n \nexamples: the list of dicts\nexample_prompt: formatted te",
    "}\",  \n    input_variables=[\"input\"]  \n)  \n \nexamples: the list of dicts\nexample_prompt: formatted template\nsuffix: suffix to add to the input\ninput_variables",
    "mplate\nsuffix: suffix to add to the input\ninput_variables",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nInvoking the few-shot prompt template\nprompt = prompt_template.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"})  \nprint(prompt.text) \nQuestion:",
    "plate.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"})  \nprint(prompt.text) \nQuestion: Does Henry Campbell have any pets? \nHenry Campbell has a dog called Pluto. \n... \n \nQuestion: What i",
    "Does Henry Campbell have any pets? \nHenry Campbell has a dog called Pluto. \n... \n \nQuestion: What is the name of Henry Campbell's dog?",
    "s the name of Henry Campbell's dog?",
    "DEVELOPING LLM APPLICATIONS WITH LANGCHAIN\nIntegration with a chain\nThe name of Henry Campbell's dog is Pluto. llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"...\")  \nllm_chain = prompt_template | llm",
    "is Pluto. llm = ChatOpenAI(model=\"gpt-4o-mini\", api_key=\"...\")  \nllm_chain = prompt_template | llm \nresponse = llm_chain.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"}) \nprint(response",
    "response = llm_chain.invoke({\"input\": \"What is the name of Henry Campbell's dog?\"}) \nprint(response.content)",
    ".content)",
    "Let's practice!\nDEVELOPING LLM APPLICATIONS WITH LANGCHAIN"
]